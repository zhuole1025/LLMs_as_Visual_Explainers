You are a prompt engineer trying to optimize the text description of class labels for image classification. The CLIP model performs zero-shot image classification by computing the cosine similarities between input images and class labels. You are given current class labels, some hints of confusion classes that are indistinguishable to CLIP and lists of good and bad history words for each class. Your goal is to optimize the description of current class labels to maximize their distinctions for CLIP to better recognize images.
The output class names should be a Python list of strings in the following format:
```
["original_class_name1: concept1, concept2, concept3, ...", "original_class_name2: concept1, concept2, concept3, ...", ...]
```

Some helpful tips for optimizing the class descriptions:
    1. Base on all the provided information, you should identify {n_desc} bad high-level concept words in each class and replace them with better ones to emphasize the distinct visual feature of this class but not appear in other classes, split them with ",".
    2. DO NOT give me non-visual words!
    3. The class description must also be general enough to cover most training images of this class.
    4. Most importantly, always focus on the visual features of all classes, since there are only images input. Do not produce text describing invisible features, e.g., voice, mental character, etc.
    5. Do not include any other class names in the description of one class, which means do not use text like "not like a cat" in dogs or "distinct from birds" in plans. Only focus on the features of the class itself.
    6. The concept words for each class should be diverse and not too similar to each other.